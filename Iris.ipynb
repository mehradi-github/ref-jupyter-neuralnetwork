{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98924921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train loss 0.6343, acc 0.800 | val loss 0.6836, acc 0.700\n",
      "Epoch  10 | train loss 0.1265, acc 0.950 | val loss 0.1548, acc 0.933\n",
      "Epoch  20 | train loss 0.0583, acc 0.975 | val loss 0.1078, acc 0.933\n",
      "Epoch  30 | train loss 0.0416, acc 0.983 | val loss 0.1038, acc 0.933\n",
      "Epoch  40 | train loss 0.0329, acc 0.992 | val loss 0.1219, acc 0.900\n",
      "Epoch  50 | train loss 0.0252, acc 0.992 | val loss 0.1312, acc 0.967\n",
      "Epoch  60 | train loss 0.0195, acc 1.000 | val loss 0.1378, acc 0.933\n",
      "Epoch  70 | train loss 0.0168, acc 0.992 | val loss 0.1710, acc 0.967\n",
      "Epoch  80 | train loss 0.0100, acc 1.000 | val loss 0.1786, acc 0.933\n",
      "Epoch  90 | train loss 0.0075, acc 1.000 | val loss 0.2002, acc 0.933\n",
      "Epoch 100 | train loss 0.0057, acc 1.000 | val loss 0.2198, acc 0.933\n",
      "Epoch 110 | train loss 0.0043, acc 1.000 | val loss 0.2242, acc 0.933\n",
      "Epoch 120 | train loss 0.0035, acc 1.000 | val loss 0.2427, acc 0.933\n",
      "Epoch 130 | train loss 0.0029, acc 1.000 | val loss 0.2627, acc 0.933\n",
      "Epoch 140 | train loss 0.0022, acc 1.000 | val loss 0.2775, acc 0.933\n",
      "Epoch 150 | train loss 0.0019, acc 1.000 | val loss 0.2823, acc 0.933\n",
      "Epoch 160 | train loss 0.0014, acc 1.000 | val loss 0.2905, acc 0.933\n",
      "Epoch 170 | train loss 0.0012, acc 1.000 | val loss 0.3001, acc 0.933\n",
      "Epoch 180 | train loss 0.0010, acc 1.000 | val loss 0.2947, acc 0.933\n",
      "Epoch 190 | train loss 0.0009, acc 1.000 | val loss 0.3086, acc 0.933\n",
      "Epoch 200 | train loss 0.0007, acc 1.000 | val loss 0.3177, acc 0.933\n",
      "Epoch 210 | train loss 0.0006, acc 1.000 | val loss 0.3125, acc 0.933\n",
      "Epoch 220 | train loss 0.0006, acc 1.000 | val loss 0.3192, acc 0.933\n",
      "Epoch 230 | train loss 0.0005, acc 1.000 | val loss 0.3276, acc 0.933\n",
      "Epoch 240 | train loss 0.0004, acc 1.000 | val loss 0.3311, acc 0.933\n",
      "Epoch 250 | train loss 0.0004, acc 1.000 | val loss 0.3488, acc 0.933\n",
      "\n",
      "Final validation accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use(\"QtAgg\")\n",
    "except Exception:\n",
    "    matplotlib.use(\"TkAgg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def one_hot(y: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros((y.size, k), dtype=np.float32)\n",
    "    out[np.arange(y.size), y] = 1.0\n",
    "    return out\n",
    "\n",
    "def relu(x): return np.maximum(0.0, x)\n",
    "def relu_grad(x): return (x > 0).astype(np.float32)\n",
    "\n",
    "def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    z = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    ez = np.exp(z)\n",
    "    return ez / np.sum(ez, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs: np.ndarray, y_onehot: np.ndarray, eps=1e-12) -> float:\n",
    "    p = np.clip(probs, eps, 1.0)\n",
    "    return float(-np.mean(np.sum(y_onehot * np.log(p), axis=1)))\n",
    "\n",
    "def accuracy(probs: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    return float(np.mean(np.argmax(probs, axis=1) == y_true))\n",
    "\n",
    "\n",
    "# 2-hidden-layer MLP + AdamW-ish (decoupled weight decay)\n",
    "\n",
    "class MLP2HiddenAdamW:\n",
    "    def __init__(self, input_dim: int, h1: int, h2: int, out_dim: int, seed: int = 42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Init\n",
    "        self.W1 = (rng.standard_normal((input_dim, h1)).astype(np.float32) * np.sqrt(2.0 / input_dim))\n",
    "        self.b1 = np.zeros((1, h1), dtype=np.float32)\n",
    "        self.W2 = (rng.standard_normal((h1, h2)).astype(np.float32) * np.sqrt(2.0 / h1))\n",
    "        self.b2 = np.zeros((1, h2), dtype=np.float32)\n",
    "        self.W3 = (rng.standard_normal((h2, out_dim)).astype(np.float32) * np.sqrt(2.0 / h2))\n",
    "        self.b3 = np.zeros((1, out_dim), dtype=np.float32)\n",
    "\n",
    "        # Adam moments\n",
    "        self.mW1 = np.zeros_like(self.W1); self.vW1 = np.zeros_like(self.W1)\n",
    "        self.mb1 = np.zeros_like(self.b1); self.vb1 = np.zeros_like(self.b1)\n",
    "        self.mW2 = np.zeros_like(self.W2); self.vW2 = np.zeros_like(self.W2)\n",
    "        self.mb2 = np.zeros_like(self.b2); self.vb2 = np.zeros_like(self.b2)\n",
    "        self.mW3 = np.zeros_like(self.W3); self.vW3 = np.zeros_like(self.W3)\n",
    "        self.mb3 = np.zeros_like(self.b3); self.vb3 = np.zeros_like(self.b3)\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, X: np.ndarray):\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        a2 = relu(z2)\n",
    "        logits = a2 @ self.W3 + self.b3\n",
    "        probs = softmax(logits)\n",
    "        return probs, (X, z1, a1, z2, a2, probs)\n",
    "\n",
    "    def backward(self, cache, y_onehot: np.ndarray):\n",
    "        X, z1, a1, z2, a2, probs = cache\n",
    "        N = X.shape[0]\n",
    "\n",
    "        dlogits = (probs - y_onehot) / N\n",
    "\n",
    "        dW3 = a2.T @ dlogits\n",
    "        db3 = np.sum(dlogits, axis=0, keepdims=True)\n",
    "\n",
    "        da2 = dlogits @ self.W3.T\n",
    "        dz2 = da2 * relu_grad(z2)\n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * relu_grad(z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "    def step(self, grads, lr=0.01, wd=1e-4, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        dW1, db1, dW2, db2, dW3, db3 = grads\n",
    "        self.t += 1\n",
    "        t = self.t\n",
    "\n",
    "        # Decoupled weight decay\n",
    "        self.W1 *= (1.0 - lr * wd)\n",
    "        self.W2 *= (1.0 - lr * wd)\n",
    "        self.W3 *= (1.0 - lr * wd)\n",
    "\n",
    "        def adam(param, grad, m, v):\n",
    "            m[:] = beta1 * m + (1.0 - beta1) * grad\n",
    "            v[:] = beta2 * v + (1.0 - beta2) * (grad * grad)\n",
    "            mhat = m / (1.0 - beta1 ** t)\n",
    "            vhat = v / (1.0 - beta2 ** t)\n",
    "            param[:] = param - lr * mhat / (np.sqrt(vhat) + eps)\n",
    "\n",
    "        adam(self.W1, dW1, self.mW1, self.vW1)\n",
    "        adam(self.b1, db1, self.mb1, self.vb1)\n",
    "        adam(self.W2, dW2, self.mW2, self.vW2)\n",
    "        adam(self.b2, db2, self.mb2, self.vb2)\n",
    "        adam(self.W3, dW3, self.mW3, self.vW3)\n",
    "        adam(self.b3, db3, self.mb3, self.vb3)\n",
    "\n",
    "\n",
    "# Data\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target.astype(np.int32)\n",
    "K = len(np.unique(y))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_val = scaler.transform(X_val).astype(np.float32)\n",
    "\n",
    "y_train_oh = one_hot(y_train, K)\n",
    "y_val_oh = one_hot(y_val, K)\n",
    "\n",
    "\n",
    "# Live plot \n",
    "\n",
    "plt.ion()\n",
    "fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.canvas.manager.set_window_title(\"Iris NN Training (Live)\")\n",
    "\n",
    "ax_acc.set_title(\"Accuracy\")\n",
    "ax_acc.set_xlabel(\"Epoch\")\n",
    "ax_acc.set_ylabel(\"Accuracy\")\n",
    "ax_acc.set_ylim(0, 1.05)\n",
    "train_acc_line, = ax_acc.plot([], [], label=\"Train\")\n",
    "val_acc_line,   = ax_acc.plot([], [], label=\"Val\")\n",
    "ax_acc.grid(True)\n",
    "ax_acc.legend()\n",
    "\n",
    "ax_loss.set_title(\"Loss\")\n",
    "ax_loss.set_xlabel(\"Epoch\")\n",
    "ax_loss.set_ylabel(\"Loss\")\n",
    "train_loss_line, = ax_loss.plot([], [], label=\"Train\")\n",
    "val_loss_line,   = ax_loss.plot([], [], label=\"Val\")\n",
    "ax_loss.grid(True)\n",
    "ax_loss.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "fig.canvas.flush_events()\n",
    "\n",
    "\n",
    "def update_plot(train_accs, val_accs, train_losses, val_losses):\n",
    "    ep = len(train_accs)\n",
    "    xs = np.arange(1, ep + 1)\n",
    "\n",
    "    train_acc_line.set_data(xs, train_accs)\n",
    "    val_acc_line.set_data(xs, val_accs)\n",
    "    ax_acc.set_xlim(1, max(10, ep))\n",
    "\n",
    "    train_loss_line.set_data(xs, train_losses)\n",
    "    val_loss_line.set_data(xs, val_losses)\n",
    "    ax_loss.set_xlim(1, max(10, ep))\n",
    "\n",
    "    all_losses = np.array(train_losses + val_losses, dtype=np.float32)\n",
    "    ymin = max(0.0, float(all_losses.min()) * 0.95)\n",
    "    ymax = float(all_losses.max()) * 1.05\n",
    "    if ymax - ymin < 1e-6:\n",
    "        ymax = ymin + 1.0\n",
    "    ax_loss.set_ylim(ymin, ymax)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "model = MLP2HiddenAdamW(input_dim=X_train.shape[1], h1=16, h2=8, out_dim=K, seed=42)\n",
    "\n",
    "epochs = 250\n",
    "batch_size = 16\n",
    "lr = 0.01\n",
    "wd = 1e-4\n",
    "update_every = 2  # fewer redraws = smoother in VS Code\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    # shuffle\n",
    "    idx = rng.permutation(len(X_train))\n",
    "    Xs = X_train[idx]\n",
    "    Ys = y_train_oh[idx]\n",
    "\n",
    "    # minibatches\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        xb = Xs[start:start + batch_size]\n",
    "        yb = Ys[start:start + batch_size]\n",
    "        probs, cache = model.forward(xb)\n",
    "        grads = model.backward(cache, yb)\n",
    "        model.step(grads, lr=lr, wd=wd)\n",
    "\n",
    "    # epoch metrics (full sets; Iris is tiny, so this is fast)\n",
    "    tr_probs, _ = model.forward(X_train)\n",
    "    va_probs, _ = model.forward(X_val)\n",
    "\n",
    "    tr_loss = cross_entropy(tr_probs, y_train_oh)\n",
    "    va_loss = cross_entropy(va_probs, y_val_oh)\n",
    "    tr_acc = accuracy(tr_probs, y_train)\n",
    "    va_acc = accuracy(va_probs, y_val)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    train_accs.append(tr_acc)\n",
    "    val_accs.append(va_acc)\n",
    "\n",
    "    if ep % update_every == 0 or ep == 1:\n",
    "        update_plot(train_accs, val_accs, train_losses, val_losses)\n",
    "\n",
    "    if ep == 1 or ep % 10 == 0:\n",
    "        print(f\"Epoch {ep:3d} | train loss {tr_loss:.4f}, acc {tr_acc:.3f} | val loss {va_loss:.4f}, acc {va_acc:.3f}\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "print(f\"\\nFinal validation accuracy: {val_accs[-1]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
